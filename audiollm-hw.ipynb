{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is tested and supposted to be used in kaggle notebbok.\n",
    "\n",
    "https://www.kaggle.com/code/alexandrmaximenko/audiollm-hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from transformers import AutoModel, AutoProcessor, WhisperFeatureExtractor, WhisperModel, AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.generation import GenerationConfig\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import IPython\n",
    "import gc\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import torch.nn as nn\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "# There might me Errors like \n",
    "# \"AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\"\n",
    "# It's okay and won't effect notebook executio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# downloading helping visualizations\n",
    "! gdown https://drive.google.com/uc?id=1k8LeWGhsn1fTXbrn9JV08lKUTOJeoEwz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework: Audio Language Model Training and Evaluation (10 points)\n",
    "\n",
    "In this homework, you will implement AudioLLM training pipeline and check it correctness with overfitting experiment.\n",
    "\n",
    "Then, you'll take pretrained checkpoint and config for AudioLLM and write code for it's evaluation.\n",
    "\n",
    "\n",
    "**Good luck! üöÄ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation Pipeline (2 points)\n",
    "Implement a QA-data generation pipeline from ASR (Automatic Speech Recognition) dataset\n",
    "- Start with ASR dataset containing `(audio, transcript)` pairs\n",
    "- Use the transcript to prompt an instruction-tuned LLM to generate question based on the transcript\n",
    "- Use the transcript & question to prompt an instruction-tuned LLM to generate answer for this question based on transcript\n",
    "- Create `(audio, question, anwer)` pairs for training\n",
    "\n",
    "**Expected output**: A data generation script that transforms ASR data into QA-dataset + some generated samples\n",
    "\n",
    "Expected sample format:\n",
    "```\n",
    "{\n",
    "  \"audio_path\": \"some_audio_path.flac\",\n",
    "  \n",
    "  \"transcript\": \"So today we're going to talk about how to prepare for a marathon if you only have twelve weeks...\",\n",
    "  \n",
    "  \"question\": \"How long does the speaker say you have to prepare for the marathon?\",\n",
    "  \n",
    "  \"answer\": \"Twelve weeks.\"\n",
    "}\n",
    "```\n",
    "\n",
    "#### Grading Criteria:\n",
    "* **2 points:** Completed *<TODO>* code in the `QADataGeneratorHF` + meaningfull examples of questions/answers in the last cell, completed `check_response` if needed\n",
    "* **1 point:**: Completed *<TODO>* code in the `QADataGeneratorHF` + many bugs in examples like incorrect format / model refusals\n",
    "* **0 points:** lack of meaningful data examples / unexecutable code\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Download librispeech dev-clean data\n",
    "! wget https://openslr.elda.org/resources/12/dev-clean.tar.gz \\\n",
    "  && tar -xf dev-clean.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "DEV_CLEAN_ROOT = Path(\"/kaggle/working/LibriSpeech/dev-clean\")\n",
    "\n",
    "\n",
    "# Parse all transcript files into a dict: utt_id -> transcription\n",
    "def load_transcripts(root: Path) -> dict:\n",
    "    transcripts = {}\n",
    "    for trans_file in root.rglob(\"*.trans.txt\"):\n",
    "        with open(trans_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if not parts:\n",
    "                    continue\n",
    "                utt_id = parts[0]              # e.g. \"84-121123-0000\"\n",
    "                text = \" \".join(parts[1:])\n",
    "                transcripts[utt_id] = text\n",
    "    return transcripts\n",
    "\n",
    "transcripts = load_transcripts(DEV_CLEAN_ROOT)\n",
    "print(\"Loaded transcripts:\", len(transcripts))\n",
    "\n",
    "# Create JSONL with one entry per audio file\n",
    "output_json = Path(\"/kaggle/working/dev-clean.json\")\n",
    "librispeech_data = []\n",
    "for flac_path in sorted(DEV_CLEAN_ROOT.rglob(\"*.flac\")):\n",
    "    utt_id = flac_path.stem  # filename without .flac\n",
    "    text = transcripts.get(utt_id)\n",
    "    record = {\n",
    "        \"audio_path\": str(flac_path),    # full path inside Kaggle FS\n",
    "        \"transcription\": text,\n",
    "    }\n",
    "    librispeech_data.append(record)\n",
    "    \n",
    "with output_json.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(librispeech_data, f)\n",
    "        \n",
    "\n",
    "print(\"Wrote JSONL to:\", output_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Example of data sample\n",
    "index = torch.randint(len(librispeech_data), (1,))\n",
    "print('Data sample transcription:\\n', librispeech_data[index]['transcription'])\n",
    "IPython.display.Audio(librispeech_data[index]['audio_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BASE_SYSTEM_MESSAGE = (\n",
    "    \"You are a helpful assistant. \"\n",
    "    \"Keep your responses concise ‚Äî maximum 50 words or 2‚Äì3 sentences.\"\n",
    ")\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class QADataGeneratorHF:\n",
    "    def __init__(self, model_path: str = \"Qwen/Qwen2.5-1.5B-Instruct\"):\n",
    "        print(f\"Loading tokenizer: {model_path}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "        print(f\"Loading transformers model: {model_path}\")\n",
    "        self.device = DEVICE\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.bfloat16 if self.device == 'cuda' else torch.float32,\n",
    "            trust_remote_code=True,\n",
    "        ).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.generation_kwargs = dict(\n",
    "            temperature=0.8,\n",
    "            top_p=0.9,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "        )\n",
    "\n",
    "        self.system_message = BASE_SYSTEM_MESSAGE\n",
    "\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def generate_single(self, transcription: str) -> dict:\n",
    "        \"\"\"Generate question and answer for a given transcription.\"\"\"\n",
    "        # Step 1: Generate a question based on the transcript\n",
    "        question_prompt_messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_message},\n",
    "            {\"role\": \"user\", \"content\": f\"Based on the following transcript, generate one short question that can be answered using information from the transcript. Only output the question, nothing else.\\n\\nTranscript: {transcription}\"}\n",
    "        ]\n",
    "        question_prompt = self.tokenizer.apply_chat_template(\n",
    "            question_prompt_messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        question_enc = self.tokenizer(question_prompt, return_tensors=\"pt\", truncation=True)\n",
    "        question_input_ids = question_enc[\"input_ids\"].to(self.device)\n",
    "        question_attention_mask = question_enc[\"attention_mask\"].to(self.device)\n",
    "        \n",
    "        question_output = self.model.generate(\n",
    "            input_ids=question_input_ids,\n",
    "            attention_mask=question_attention_mask,\n",
    "            **self.generation_kwargs\n",
    "        )\n",
    "        question = self.tokenizer.decode(\n",
    "            question_output[0][question_input_ids.shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        ).strip()\n",
    "        \n",
    "        # Step 2: Generate an answer based on the transcript and question\n",
    "        answer_prompt_messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_message},\n",
    "            {\"role\": \"user\", \"content\": f\"Based on the following transcript, answer the question concisely. Only output the answer, nothing else.\\n\\nTranscript: {transcription}\\n\\nQuestion: {question}\"}\n",
    "        ]\n",
    "        answer_prompt = self.tokenizer.apply_chat_template(\n",
    "            answer_prompt_messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        answer_enc = self.tokenizer(answer_prompt, return_tensors=\"pt\", truncation=True)\n",
    "        answer_input_ids = answer_enc[\"input_ids\"].to(self.device)\n",
    "        answer_attention_mask = answer_enc[\"attention_mask\"].to(self.device)\n",
    "        \n",
    "        answer_output = self.model.generate(\n",
    "            input_ids=answer_input_ids,\n",
    "            attention_mask=answer_attention_mask,\n",
    "            **self.generation_kwargs\n",
    "        )\n",
    "        answer = self.tokenizer.decode(\n",
    "            answer_output[0][answer_input_ids.shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        ).strip()\n",
    "        \n",
    "        return {\"question\": question, \"answer\": answer}\n",
    "\n",
    "    def generate_batch(self, transcriptions: list[str]) -> list[dict]:\n",
    "        \"\"\"Generate responses for a list of transcriptions with progress bar.\"\"\"\n",
    "        responses = []\n",
    "        for transcription in tqdm(transcriptions, desc=\"Generating responses\"):\n",
    "            response = self.generate_single(transcription)\n",
    "            responses.append(response)\n",
    "        \n",
    "        return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "generator = QADataGeneratorHF(\"Qwen/Qwen2.5-1.5B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 100\n",
    "librispeech_data_sample = librispeech_data[:NUM_SAMPLES]\n",
    "transcriptions = [sample['transcription'] for sample in librispeech_data_sample]\n",
    "\n",
    "generated_responses = generator.generate_batch(transcriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check your instruct data samples\n",
    "index = torch.randint(NUM_SAMPLES, (1,))\n",
    "print('Transcription example: ', librispeech_data_sample[index]['transcription'])\n",
    "print('='*100)\n",
    "print('Question example: ', generated_responses[index]['question'])\n",
    "print('='*100)\n",
    "print('Answer example: ', generated_responses[index]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Add simple filtering\n",
    "\n",
    "def check_response(response: dict) -> bool:\n",
    "    \"\"\"Check if the generated response is valid.\"\"\"\n",
    "    # Check if response has required keys\n",
    "    if not isinstance(response, dict):\n",
    "        return False\n",
    "    if 'question' not in response or 'answer' not in response:\n",
    "        return False\n",
    "    \n",
    "    question = response.get('question', '')\n",
    "    answer = response.get('answer', '')\n",
    "    \n",
    "    # Check for empty responses\n",
    "    if not question or not answer:\n",
    "        return False\n",
    "    \n",
    "    # Check for model refusals or errors\n",
    "    rejection_patterns = [\n",
    "        \"I can't\", \"I cannot\", \"I'm not able\", \"I am not able\",\n",
    "        \"I don't have\", \"I do not have\",\n",
    "        \"It seems like\", \"I'm sorry\", \"I am sorry\",\n",
    "        \"I can't help\", \"I can't provide\", \"I can't create\",\n",
    "        \"As an AI\", \"As a language model\",\n",
    "    ]\n",
    "    for pattern in rejection_patterns:\n",
    "        if pattern.lower() in question.lower() or pattern.lower() in answer.lower():\n",
    "            return False\n",
    "    \n",
    "    # Check minimum length\n",
    "    if len(question) < 5 or len(answer) < 2:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "generated_samples = []\n",
    "rejected_samples = []\n",
    "\n",
    "for data_sample, response in zip(librispeech_data_sample[:NUM_SAMPLES], generated_responses):\n",
    "    if check_response(response):\n",
    "        data_sample.update(response)\n",
    "        generated_samples.append(data_sample)\n",
    "    else:\n",
    "        data_sample.update(response)\n",
    "        rejected_samples.append(data_sample)\n",
    "\n",
    "print('Number of samples before filtering: ', NUM_SAMPLES)\n",
    "print('Number of samples after filtering: ', len(generated_samples))\n",
    "with open('instruct_data.json', 'w') as file:\n",
    "    json.dump(generated_samples, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check rejected samples\n",
    "index = torch.randint(len(rejected_samples), (1,))\n",
    "print('Transcription example: ', rejected_samples[index]['transcription'])\n",
    "print('Response example: ', rejected_samples[index]['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check good samples\n",
    "index = torch.randint(len(generated_samples), (1,))\n",
    "print('Transcription example: ', generated_samples[index]['transcription'])\n",
    "print('='*100)\n",
    "print('Question example: ', generated_samples[index]['question'])\n",
    "print('='*100)\n",
    "print('Answer example: ', generated_samples[index]['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Grading Criteria:\n",
    "* **2 points:** Completed *<TODO>* code in the `QADataGeneratorHF` + meaningfull examples of questions/answers in the last cell, completed `check_response` if needed\n",
    "* **1 point:**: Completed *<TODO>* code in the `QADataGeneratorHF` + many bugs in examples like incorrect format / model refusals\n",
    "* **0 points:** lack of meaningful data examples / unexecutable code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. AudioLLM Model & Dataset & Overfitting (4 points)\n",
    "\n",
    "In this section you'll need to: \n",
    "* Implement AudioLLM model with architecture, similar to UltraVox.\n",
    "\n",
    "* Implement dataset class for this model based on QA-samples you've generated earlier\n",
    "\n",
    "* Run overfitting experiment with your model and your dataset\n",
    "\n",
    "\n",
    "#### Grading Criteria:\n",
    "* **+2 points:** Completed *<TODO>* code in the `AudioAdapter` and in the `AudioLLM`, `AudioEmbeddingInsertionTests` passed\n",
    "* **+1 point:** Completed *<TODO>* code in the `AudioInstructDataset` + demonstration of dataset samples\n",
    "* **+1 point:** Reaching `loss < 0.2` on your overfit experiment\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let start with model.\n",
    "\n",
    "**Model components**:\n",
    "  - Audio encoder - Whisper\n",
    "  - Audio Adapter - Linear with subsampling\n",
    "  - Language model\n",
    "\n",
    "Your first task - to fill **`#<TODO>`** parts in AudioAdapter and AudioLLM classes.\n",
    "\n",
    "##### Grading Criteria:\n",
    "* **2 points:** Completed *<TODO>* code in the `AudioAdapter` and in the `AudioLLM`, `AudioEmbeddingInsertionTests` passed\n",
    "* **1 point:** Completed *<TODO>* code in the `AudioAdapter`, `AudioEmbeddingInsertionTests` failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for the Audio LLM model.\"\"\"\n",
    "    \n",
    "    whisper_model: str = \"openai/whisper-small\"\n",
    "    llm_model: str = \"Qwen/Qwen3-0.6B\"\n",
    "    \n",
    "    # Audio adapter\n",
    "    adapter_hidden_dim: int = 1024 # corresponding to llm input dimension\n",
    "    adapter_num_layers: int = 2\n",
    "    adapter_dropout: float = 0.1\n",
    "    subsample_factor: int = 4\n",
    "\n",
    "    # Training strategy\n",
    "    freeze_whisper: bool = True  # Freeze Whisper encoder\n",
    "    freeze_llm: bool = True  # Set to True for adapter-only training (no LoRA needed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AudioAdapter(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        whisper_dim: int,\n",
    "        llm_dim: int,\n",
    "        hidden_dim: int = 2048,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.1,\n",
    "        subsample_factor: int = 1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Simple MLP Adapter with configurable subsample_factor\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.subsample_factor = subsample_factor\n",
    "\n",
    "        # Set input_dim according to subsample_factor and whisper_dim\n",
    "        input_dim = whisper_dim * subsample_factor\n",
    "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "            for _ in range(num_layers - 1)\n",
    "        ])\n",
    "\n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, llm_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, whisper_features: torch.Tensor) -> torch.Tensor:\n",
    "        if self.subsample_factor > 1:\n",
    "            batch_size, seq_len, dim = whisper_features.shape\n",
    "            remainder = seq_len % self.subsample_factor\n",
    "            if remainder != 0:\n",
    "                whisper_features = whisper_features[:, :-remainder, :]\n",
    "                seq_len = whisper_features.shape[1]\n",
    "            new_seq_len = seq_len // self.subsample_factor\n",
    "            x = whisper_features.reshape(batch_size, new_seq_len, dim * self.subsample_factor)\n",
    "        else:\n",
    "            x = whisper_features\n",
    "\n",
    "        # Applying input projection\n",
    "        x = self.input_proj(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = x + layer(x)\n",
    "\n",
    "        # Applying output projection\n",
    "        x = self.output_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "AUDIO_TOKEN = \"<|audio_token|>\"\n",
    "\n",
    "class AudioLLM(nn.Module):\n",
    "    \"\"\"\n",
    "    Audio Language Model combining Whisper encoder with a text LLM.\n",
    "    \n",
    "    Architecture:\n",
    "    1. Whisper encoder processes audio\n",
    "    2. Audio adapter projects features to LLM space\n",
    "    3. LLM generates text conditioned on audio features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.model_config = config\n",
    "        \n",
    "        # Load Whisper encoder\n",
    "        print(f\"Loading Whisper model: {config.whisper_model}\")\n",
    "        whisper = WhisperModel.from_pretrained(config.whisper_model)\n",
    "        self.whisper_encoder = whisper.encoder\n",
    "        whisper_dim = self.whisper_encoder.config.d_model\n",
    "        \n",
    "        if config.freeze_whisper:\n",
    "            for param in self.whisper_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"Whisper encoder frozen\")\n",
    "        \n",
    "        # Load LLM\n",
    "        print(f\"Loading LLM: {config.llm_model}\")\n",
    "        self.llm = AutoModelForCausalLM.from_pretrained(\n",
    "            config.llm_model,\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "\n",
    "        # Adding special audio token\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.llm_model)\n",
    "        self.tokenizer.add_special_tokens({\"additional_special_tokens\": [AUDIO_TOKEN]})\n",
    "        self.audio_token_id = self.tokenizer(AUDIO_TOKEN, add_special_tokens=False).input_ids[0]\n",
    "        \n",
    "        # Ensure tokenizer has pad token\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "        llm_dim = self.llm.config.hidden_size\n",
    "        \n",
    "        # Audio adapter (initialize before freezing/LoRA)\n",
    "        print(\"Initializing audio adapter\")\n",
    "        self.audio_adapter = AudioAdapter(\n",
    "            whisper_dim=whisper_dim,\n",
    "            llm_dim=llm_dim,\n",
    "            hidden_dim=config.adapter_hidden_dim,\n",
    "            num_layers=config.adapter_num_layers,\n",
    "            dropout=config.adapter_dropout,\n",
    "            subsample_factor=config.subsample_factor\n",
    "        )\n",
    "        \n",
    "        # Freeze LLM if specified\n",
    "        if config.freeze_llm:\n",
    "            for param in self.llm.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"‚úì LLM fully frozen (only adapter will be trainable)\")\n",
    "        \n",
    "        # Set config attribute to LLM's config (required by Trainer)\n",
    "        self.config = self.llm.config\n",
    "        \n",
    "    def get_device(self):\n",
    "        return next(self.llm.parameters()).device\n",
    "\n",
    "    def get_dtype(self):\n",
    "        return next(self.llm.parameters()).dtype\n",
    "    \n",
    "    def encode_audio(self, audio_values: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode audio through Whisper and adapter.\n",
    "        For simplification, we are using fixed 30s-padding \n",
    "        as in original whisper and won't use audios longer \n",
    "        than 30 seconds in our training.\n",
    "        \n",
    "        Args:\n",
    "            audio_values: [batch_size, audio_length]\n",
    "        Returns:\n",
    "            audio_embeds: [batch_size, num_audio_tokens, llm_dim]\n",
    "        \"\"\"\n",
    "        # Ensure audio is on correct device and dtype\n",
    "        whisper_dtype = self.get_dtype()\n",
    "        whisper_device = self.get_device()\n",
    "        audio_values = audio_values.to(device=whisper_device, dtype=whisper_dtype)\n",
    "        \n",
    "        # Whisper encoding\n",
    "        with torch.no_grad() if self.model_config.freeze_whisper else torch.enable_grad():\n",
    "            whisper_outputs = self.whisper_encoder(audio_values)\n",
    "            whisper_features = whisper_outputs.last_hidden_state\n",
    "        \n",
    "        # Project to LLM space\n",
    "        audio_embeds = self.audio_adapter(whisper_features)\n",
    "        return audio_embeds\n",
    "    \n",
    "    @staticmethod\n",
    "    def insert_audio_embeds(\n",
    "        audio_embeds: torch.Tensor, \n",
    "        text_embeds: torch.Tensor, \n",
    "        input_ids: torch.Tensor,\n",
    "        labels: Optional[torch.Tensor],\n",
    "        audio_token_id: int\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Insert audio embeddings into text embeddings at positions marked by audio_token_id.\n",
    "        \n",
    "        Args:\n",
    "            audio_embeds: [batch_size, num_audio_tokens, llm_dim]\n",
    "            text_embeds: [batch_size, text_seq_len, llm_dim]\n",
    "            input_ids: [batch_size, text_seq_len] - contains audio_token_id at positions to replace\n",
    "            labels: [batch_size, text_seq_len] - optional labels to adjust\n",
    "            audio_token_id: int - ID of the audio token placeholder\n",
    "        \n",
    "        Returns:\n",
    "            combined_embeds: [batch_size, new_seq_len, llm_dim] where audio_token_id positions are replaced\n",
    "            combined_labels: [batch_size, new_seq_len] with -100 at audio positions (if labels provided)\n",
    "        \"\"\"\n",
    "        batch_size, num_audio_tokens, llm_dim = audio_embeds.shape\n",
    "        text_seq_len = text_embeds.shape[1]\n",
    "        device = audio_embeds.device\n",
    "        dtype = audio_embeds.dtype\n",
    "        \n",
    "        combined_embeds_list = []\n",
    "        combined_labels_list = []\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            # Find the position of audio_token_id in this batch item\n",
    "            audio_token_positions = (input_ids[b] == audio_token_id).nonzero(as_tuple=True)[0]\n",
    "            \n",
    "            if len(audio_token_positions) == 0:\n",
    "                # No audio token found, just use text embeddings as-is\n",
    "                combined_embeds_list.append(text_embeds[b])\n",
    "                if labels is not None:\n",
    "                    combined_labels_list.append(labels[b])\n",
    "                continue\n",
    "            \n",
    "            # Get the position of the (first) audio token placeholder\n",
    "            audio_pos = audio_token_positions[0].item()\n",
    "            \n",
    "            # Build combined embeddings:\n",
    "            # [text_before_audio] + [audio_embeds] + [text_after_audio]\n",
    "            text_before = text_embeds[b, :audio_pos]  # [audio_pos, dim]\n",
    "            text_after = text_embeds[b, audio_pos + 1:]  # [text_seq_len - audio_pos - 1, dim]\n",
    "            audio = audio_embeds[b]  # [num_audio_tokens, dim]\n",
    "            \n",
    "            combined = torch.cat([text_before, audio, text_after], dim=0)\n",
    "            combined_embeds_list.append(combined)\n",
    "            \n",
    "            # Build combined labels if provided\n",
    "            if labels is not None:\n",
    "                labels_before = labels[b, :audio_pos]  # [audio_pos]\n",
    "                labels_after = labels[b, audio_pos + 1:]  # [text_seq_len - audio_pos - 1]\n",
    "                # Audio positions should have -100 (ignored in loss)\n",
    "                audio_labels = torch.full((num_audio_tokens,), -100, device=device, dtype=labels.dtype)\n",
    "                \n",
    "                combined_labels = torch.cat([labels_before, audio_labels, labels_after], dim=0)\n",
    "                combined_labels_list.append(combined_labels)\n",
    "        \n",
    "        # Stack all batch items (they should all have the same length now)\n",
    "        combined_embeds = torch.stack(combined_embeds_list, dim=0)\n",
    "        \n",
    "        if labels is not None:\n",
    "            combined_labels = torch.stack(combined_labels_list, dim=0)\n",
    "        else:\n",
    "            combined_labels = None\n",
    "        \n",
    "        return combined_embeds, combined_labels\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        audio_values: torch.Tensor,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Forward pass for training.\n",
    "        \n",
    "        Args:\n",
    "            audio_values: [batch_size, audio_length]\n",
    "            input_ids: [batch_size, text_seq_len] - text tokens\n",
    "            attention_mask: [batch_size, text_seq_len]\n",
    "            labels: [batch_size, text_seq_len] - for computing loss\n",
    "        \"\"\"\n",
    "        batch_size = audio_values.shape[0]\n",
    "        \n",
    "        # Encode audio\n",
    "        audio_embeds = self.encode_audio(audio_values)  # [B, audio_tokens, dim]\n",
    "        num_audio_tokens = audio_embeds.shape[1]\n",
    "        \n",
    "        # Get text embeddings\n",
    "        text_embeds = self.llm.get_input_embeddings()(input_ids)  # [B, text_tokens, dim]\n",
    "        \n",
    "        # Insert audio embeddings and adjust labels\n",
    "        combined_embeds, combined_labels = AudioLLM.insert_audio_embeds(\n",
    "            audio_embeds, text_embeds, input_ids, labels, self.audio_token_id\n",
    "        )\n",
    "        \n",
    "        # Create combined attention mask\n",
    "        if attention_mask is not None:\n",
    "            # Prepend ones for audio embeddings to existing attention mask\n",
    "            audio_attention = torch.ones(\n",
    "                batch_size, num_audio_tokens,\n",
    "                device=audio_embeds.device,\n",
    "                dtype=attention_mask.dtype\n",
    "            )\n",
    "            combined_attention = torch.cat([audio_attention, attention_mask], dim=1)\n",
    "        else:\n",
    "            combined_attention = None\n",
    "        \n",
    "        # Forward through LLM\n",
    "        outputs = self.llm(\n",
    "            inputs_embeds=combined_embeds,\n",
    "            attention_mask=combined_attention,\n",
    "            labels=combined_labels,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio Embedding Insertion Method\n",
    "\n",
    "In the whole AudioLLM class you'll need to implement only this method, all other methods are implemented already.\n",
    "\n",
    "This method is marked as static to make automatic testing easier, don't unmark it.\n",
    "\n",
    "##### Overview\n",
    "\n",
    "This method is the core of multimodal fusion in AudioLLM. It takes:\n",
    "- **Text embeddings** with special audio placeholder tokens\n",
    "- **Audio embeddings** from the audio encoder\n",
    "- **Input IDs** showing where placeholders are located\n",
    "\n",
    "And produces a **combined embedding sequence** where audio placeholders are replaced by actual audio embeddings.\n",
    "\n",
    "##### Algorithm Steps\n",
    "\n",
    "1. **Locate placeholders**: Find positions in `input_ids` that contain `audio_token_id`\n",
    "2. **Replace with audio**: At each placeholder position, insert ALL audio embeddings\n",
    "3. **Keep text embeddings**: All other positions keep their original text embeddings\n",
    "4. **Mask labels**: Set labels to `-100` at audio positions (ignored in loss calculation)\n",
    "5. **Pad sequences**: Ensure all batch items have the same length\n",
    "\n",
    "##### Visual Example\n",
    "\n",
    "See the diagram below for how the insertion works:\n",
    "![](embeddings_visualization.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AudioEmbeddingInsertionTests:\n",
    "    \"\"\"Test suite for insert_audio_embeds method.\"\"\"\n",
    "    \n",
    "    def __init__(self, insert_audio_embeds_func):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            insert_audio_embeds_func: The static method to test\n",
    "        \"\"\"\n",
    "        self.insert_func = insert_audio_embeds_func\n",
    "        self.audio_token_id = 99999\n",
    "        self.llm_dim = 128\n",
    "        \n",
    "    def test_single_audio_token_per_sample(self):\n",
    "        \"\"\"Test with exactly one audio placeholder per sample.\"\"\"\n",
    "        print(\"\\n‚úì Test 1: Single audio placeholder per sample\")\n",
    "        \n",
    "        batch_size = 2\n",
    "        text_seq_len = 5\n",
    "        num_audio_tokens = 15  # Typical for Ultravox\n",
    "        \n",
    "        audio_embeds = torch.randn(batch_size, num_audio_tokens, self.llm_dim)\n",
    "        text_embeds = torch.randn(batch_size, text_seq_len, self.llm_dim)\n",
    "        \n",
    "        # Each sample has exactly 1 audio placeholder at different positions\n",
    "        input_ids = torch.tensor([\n",
    "            [1, 2, self.audio_token_id, 3, 4],      # Audio at position 2\n",
    "            [5, 6, 7, self.audio_token_id, 8]       # Audio at position 3\n",
    "        ])\n",
    "        \n",
    "        labels = torch.tensor([\n",
    "            [10, 20, 30, 40, 50],\n",
    "            [60, 70, 80, 90, 100]\n",
    "        ])\n",
    "        \n",
    "        combined_embeds, combined_labels = self.insert_func(\n",
    "            audio_embeds, text_embeds, input_ids, labels, self.audio_token_id\n",
    "        )\n",
    "        \n",
    "        # Expected: 5 - 1 + 15 = 19 tokens per sequence\n",
    "        expected_len = text_seq_len - 1 + num_audio_tokens\n",
    "        assert combined_embeds.shape == (batch_size, expected_len, self.llm_dim), \\\n",
    "            f\"Expected shape {(batch_size, expected_len, self.llm_dim)}, got {combined_embeds.shape}\"\n",
    "        assert combined_labels.shape == (batch_size, expected_len), \\\n",
    "            f\"Expected labels shape {(batch_size, expected_len)}, got {combined_labels.shape}\"\n",
    "        \n",
    "        # Check audio positions have -100 labels\n",
    "        # Batch 0: audio at positions 2-16 (15 tokens)\n",
    "        for i in range(2, 2 + num_audio_tokens):\n",
    "            assert combined_labels[0, i] == -100, \\\n",
    "                f\"Audio position {i} should have -100 label, got {combined_labels[0, i]}\"\n",
    "        \n",
    "        # Batch 1: audio at positions 3-17 (15 tokens)\n",
    "        for i in range(3, 3 + num_audio_tokens):\n",
    "            assert combined_labels[1, i] == -100, \\\n",
    "                f\"Audio position {i} should have -100 label, got {combined_labels[1, i]}\"\n",
    "        \n",
    "        # Check non-audio positions keep original labels\n",
    "        assert combined_labels[0, 0] == 10, \"First position should keep original label\"\n",
    "        assert combined_labels[0, 1] == 20, \"Second position should keep original label\"\n",
    "        assert combined_labels[0, 17] == 40, \"Position after audio should keep original label\"\n",
    "        \n",
    "        print(f\"  ‚úì Output shape: {combined_embeds.shape}\")\n",
    "        print(f\"  ‚úì Labels shape: {combined_labels.shape}\")\n",
    "        print(f\"  ‚úì Audio positions (15 tokens) masked with -100\")\n",
    "        print(f\"  ‚úì Non-audio positions preserve original labels\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def test_audio_at_beginning(self):\n",
    "        \"\"\"Test with audio placeholder at the start of sequence.\"\"\"\n",
    "        print(\"\\n‚úì Test 2: Audio placeholder at sequence beginning\")\n",
    "        \n",
    "        batch_size = 2\n",
    "        text_seq_len = 6\n",
    "        num_audio_tokens = 10\n",
    "        \n",
    "        audio_embeds = torch.randn(batch_size, num_audio_tokens, self.llm_dim)\n",
    "        text_embeds = torch.randn(batch_size, text_seq_len, self.llm_dim)\n",
    "        \n",
    "        # Audio at position 0\n",
    "        input_ids = torch.tensor([\n",
    "            [self.audio_token_id, 1, 2, 3, 4, 5],\n",
    "            [self.audio_token_id, 6, 7, 8, 9, 10]\n",
    "        ])\n",
    "        \n",
    "        labels = torch.tensor([\n",
    "            [100, 10, 20, 30, 40, 50],\n",
    "            [200, 60, 70, 80, 90, 100]\n",
    "        ])\n",
    "        \n",
    "        combined_embeds, combined_labels = self.insert_func(\n",
    "            audio_embeds, text_embeds, input_ids, labels, self.audio_token_id\n",
    "        )\n",
    "        \n",
    "        expected_len = text_seq_len - 1 + num_audio_tokens\n",
    "        assert combined_embeds.shape == (batch_size, expected_len, self.llm_dim), \\\n",
    "            f\"Expected shape {(batch_size, expected_len, self.llm_dim)}, got {combined_embeds.shape}\"\n",
    "        \n",
    "        # First 10 positions should be -100\n",
    "        for i in range(num_audio_tokens):\n",
    "            assert combined_labels[0, i] == -100, \\\n",
    "                f\"Audio position {i} at beginning should have -100\"\n",
    "        \n",
    "        # Check that text labels follow after audio\n",
    "        assert combined_labels[0, num_audio_tokens] == 10, \\\n",
    "            \"First text label should appear after audio\"\n",
    "        \n",
    "        print(f\"  ‚úì Audio at beginning handled correctly\")\n",
    "        print(f\"  ‚úì Text labels follow after audio embeddings\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def test_audio_at_end(self):\n",
    "        \"\"\"Test with audio placeholder at the end of sequence.\"\"\"\n",
    "        print(\"\\n‚úì Test 3: Audio placeholder at sequence end\")\n",
    "        \n",
    "        batch_size = 2\n",
    "        text_seq_len = 6\n",
    "        num_audio_tokens = 10\n",
    "        \n",
    "        audio_embeds = torch.randn(batch_size, num_audio_tokens, self.llm_dim)\n",
    "        text_embeds = torch.randn(batch_size, text_seq_len, self.llm_dim)\n",
    "        \n",
    "        # Audio at last position\n",
    "        input_ids = torch.tensor([\n",
    "            [1, 2, 3, 4, 5, self.audio_token_id],\n",
    "            [6, 7, 8, 9, 10, self.audio_token_id]\n",
    "        ])\n",
    "        \n",
    "        labels = torch.tensor([\n",
    "            [10, 20, 30, 40, 50, 100],\n",
    "            [60, 70, 80, 90, 100, 200]\n",
    "        ])\n",
    "        \n",
    "        combined_embeds, combined_labels = self.insert_func(\n",
    "            audio_embeds, text_embeds, input_ids, labels, self.audio_token_id\n",
    "        )\n",
    "        \n",
    "        expected_len = text_seq_len - 1 + num_audio_tokens\n",
    "        assert combined_embeds.shape == (batch_size, expected_len, self.llm_dim), \\\n",
    "            f\"Expected shape {(batch_size, expected_len, self.llm_dim)}, got {combined_embeds.shape}\"\n",
    "        \n",
    "        # Last 10 positions should be -100\n",
    "        for i in range(expected_len - num_audio_tokens, expected_len):\n",
    "            assert combined_labels[0, i] == -100, \\\n",
    "                f\"Audio position {i} at end should have -100\"\n",
    "        \n",
    "        # Check that text labels come before audio\n",
    "        assert combined_labels[0, 0] == 10, \"First text label preserved\"\n",
    "        assert combined_labels[0, 4] == 50, \"Last text label before audio preserved\"\n",
    "        \n",
    "        print(f\"  ‚úì Audio at end handled correctly\")\n",
    "        print(f\"  ‚úì Text labels preserved before audio\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def test_embedding_values_preserved(self):\n",
    "        \"\"\"Test that text and audio embeddings are correctly preserved.\"\"\"\n",
    "        print(\"\\n‚úì Test 4: Embedding values preservation\")\n",
    "        \n",
    "        batch_size = 1\n",
    "        text_seq_len = 4\n",
    "        num_audio_tokens = 3\n",
    "        \n",
    "        # Create embeddings with known values\n",
    "        audio_embeds = torch.ones(batch_size, num_audio_tokens, self.llm_dim) * 100\n",
    "        text_embeds = torch.ones(batch_size, text_seq_len, self.llm_dim) * 50\n",
    "        \n",
    "        # Audio at position 2\n",
    "        input_ids = torch.tensor([[1, 2, self.audio_token_id, 3]])\n",
    "        labels = torch.tensor([[10, 20, 30, 40]])\n",
    "        \n",
    "        combined_embeds, combined_labels = self.insert_func(\n",
    "            audio_embeds, text_embeds, input_ids, labels, self.audio_token_id\n",
    "        )\n",
    "        \n",
    "        # Check text embeddings before audio\n",
    "        assert torch.allclose(combined_embeds[0, 0], text_embeds[0, 0]), \\\n",
    "            \"Text embedding at position 0 should be preserved\"\n",
    "        assert torch.allclose(combined_embeds[0, 1], text_embeds[0, 1]), \\\n",
    "            \"Text embedding at position 1 should be preserved\"\n",
    "        \n",
    "        # Check audio embeddings\n",
    "        for i in range(num_audio_tokens):\n",
    "            assert torch.allclose(combined_embeds[0, 2 + i], audio_embeds[0, i]), \\\n",
    "                f\"Audio embedding {i} should be preserved at position {2 + i}\"\n",
    "        \n",
    "        # Check text embedding after audio\n",
    "        assert torch.allclose(combined_embeds[0, 5], text_embeds[0, 3]), \\\n",
    "            \"Text embedding after audio should be preserved\"\n",
    "        \n",
    "        print(f\"  ‚úì Text embeddings preserved correctly\")\n",
    "        print(f\"  ‚úì Audio embeddings inserted correctly\")\n",
    "        print(f\"  ‚úì Embedding values match expected\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def test_batch_consistency(self):\n",
    "        \"\"\"Test that batch processing is consistent.\"\"\"\n",
    "        print(\"\\n‚úì Test 5: Batch consistency\")\n",
    "        \n",
    "        batch_size = 3\n",
    "        text_seq_len = 7\n",
    "        num_audio_tokens = 15\n",
    "        \n",
    "        audio_embeds = torch.randn(batch_size, num_audio_tokens, self.llm_dim)\n",
    "        text_embeds = torch.randn(batch_size, text_seq_len, self.llm_dim)\n",
    "        \n",
    "        # Each sample has audio at position 3\n",
    "        input_ids = torch.tensor([\n",
    "            [1, 2, 3, self.audio_token_id, 4, 5, 6],\n",
    "            [7, 8, 9, self.audio_token_id, 10, 11, 12],\n",
    "            [13, 14, 15, self.audio_token_id, 16, 17, 18]\n",
    "        ])\n",
    "        \n",
    "        labels = torch.tensor([\n",
    "            [10, 20, 30, 40, 50, 60, 70],\n",
    "            [80, 90, 100, 110, 120, 130, 140],\n",
    "            [150, 160, 170, 180, 190, 200, 210]\n",
    "        ])\n",
    "        \n",
    "        combined_embeds, combined_labels = self.insert_func(\n",
    "            audio_embeds, text_embeds, input_ids, labels, self.audio_token_id\n",
    "        )\n",
    "        \n",
    "        expected_len = text_seq_len - 1 + num_audio_tokens\n",
    "        \n",
    "        # All batch items should have same length\n",
    "        assert combined_embeds.shape[0] == batch_size, \\\n",
    "            f\"Batch size should be {batch_size}\"\n",
    "        assert combined_embeds.shape[1] == expected_len, \\\n",
    "            f\"All sequences should have length {expected_len}\"\n",
    "        \n",
    "        # Check each batch item has audio masked correctly\n",
    "        for b in range(batch_size):\n",
    "            audio_start = 3\n",
    "            for i in range(audio_start, audio_start + num_audio_tokens):\n",
    "                assert combined_labels[b, i] == -100, \\\n",
    "                    f\"Batch {b}, position {i} should be -100\"\n",
    "        \n",
    "        print(f\"  ‚úì All batch items have consistent length: {expected_len}\")\n",
    "        print(f\"  ‚úì Audio masking consistent across batch\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def run_all_tests(self):\n",
    "        \"\"\"Run all tests and report results.\"\"\"\n",
    "        print(\"=\"*70)\n",
    "        print(\"Running Audio Embedding Insertion Tests\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        tests = [\n",
    "            (\"Single Audio Token Per Sample\", self.test_single_audio_token_per_sample),\n",
    "            (\"Audio at Beginning\", self.test_audio_at_beginning),\n",
    "            (\"Audio at End\", self.test_audio_at_end),\n",
    "            (\"Embedding Values Preservation\", self.test_embedding_values_preserved),\n",
    "            (\"Batch Consistency\", self.test_batch_consistency),\n",
    "        ]\n",
    "        \n",
    "        passed = 0\n",
    "        failed = 0\n",
    "        \n",
    "        for test_name, test_func in tests:\n",
    "            try:\n",
    "                result = test_func()\n",
    "                if result:\n",
    "                    passed += 1\n",
    "            except AssertionError as e:\n",
    "                print(f\"  ‚úó FAILED: {e}\")\n",
    "                failed += 1\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚úó ERROR: {e}\")\n",
    "                failed += 1\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"Test Results: {passed}/{len(tests)} passed\")\n",
    "        if failed == 0:\n",
    "            print(\"‚úÖ All tests passed! Your implementation is correct.\")\n",
    "        else:\n",
    "            print(f\"‚ùå {failed} test(s) failed. Please review your implementation.\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        return failed == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_suite = AudioEmbeddingInsertionTests(AudioLLM.insert_audio_embeds)\n",
    "test_suite.run_all_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Implementation\n",
    "* We've implemented the model class at this point, it's time to implement Audio-based QA dataset, samples for which we've generated earlier\n",
    "* Your goal, again - fill **`<TODO>`**'s\n",
    "\n",
    "##### Grading Criteria:\n",
    "* **1 point:** Completed *<TODO>* code in the `AudioInstructDataset` + demonstration of dataset samples with **correct** labels masking\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_prompt(\n",
    "        tokenizer: AutoTokenizer,\n",
    "        instruction: str,\n",
    "        response: str,\n",
    "        system_message: str = BASE_SYSTEM_MESSAGE\n",
    ") -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message}\n",
    "    ]\n",
    "    messages.append({\"role\": \"user\", \"content\": instruction})\n",
    "\n",
    "    if response is not None:\n",
    "        messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "        add_generation_prompt = False\n",
    "    else:\n",
    "        add_generation_prompt = True\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=add_generation_prompt\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "class AudioInstructDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        json_path: str,\n",
    "        tokenizer,\n",
    "        feature_extractor: WhisperFeatureExtractor,\n",
    "        max_length: int = 512,\n",
    "        is_generation_set: bool = False,\n",
    "        num_samples: Optional[int] = None,\n",
    "    ):\n",
    "        with open(json_path, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "\n",
    "        if num_samples is not None:\n",
    "            self.data = self.data[:num_samples]\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.max_length = max_length\n",
    "        self.is_generation_set = is_generation_set\n",
    "\n",
    "        print(f\"Loaded {len(self.data)} samples from {json_path}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx) -> dict:\n",
    "        sample = self.data[idx]\n",
    "\n",
    "        # Load audio from file path and resample to 16kHz if needed\n",
    "        audio_path = sample[\"audio_path\"]\n",
    "        waveform, sr = librosa.load(audio_path, sr=16000)\n",
    "        waveform = torch.tensor(waveform).unsqueeze(0)\n",
    "\n",
    "        # Process audio with Whisper feature extractor\n",
    "        audio_array = waveform.squeeze(0).numpy()\n",
    "        audio_inputs = self.feature_extractor(\n",
    "            audio_array,\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        audio_values = audio_inputs.input_features.squeeze(0)\n",
    "\n",
    "        # Create chat-formatted prompt with instruction and response\n",
    "        instruction = f\"{AUDIO_TOKEN}Based on the given audio, answer the question: {sample['question']}\"\n",
    "        response = sample[\"answer\"]\n",
    "\n",
    "        text = create_prompt(\n",
    "            self.tokenizer,\n",
    "            instruction=instruction,\n",
    "            response=response,\n",
    "        )\n",
    "\n",
    "        # Tokenize\n",
    "        tokenized = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"longest\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = tokenized.input_ids.squeeze(0)\n",
    "        attention_mask = tokenized.attention_mask.squeeze(0)\n",
    "\n",
    "        # Mask tokens corresponding to system and user replic with -100 value\n",
    "        # Find the position where the assistant response starts\n",
    "        # We need to mask everything before the assistant's response (system + user messages)\n",
    "        # For Qwen models, we look for the assistant header or similar pattern\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        # Create prompt without response to find where assistant response starts\n",
    "        prompt_without_response = create_prompt(\n",
    "            self.tokenizer,\n",
    "            instruction=instruction,\n",
    "            response=None,  # No response to get just the prompt\n",
    "        )\n",
    "        prompt_tokens = self.tokenizer(\n",
    "            prompt_without_response,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=False\n",
    "        ).input_ids.squeeze(0)\n",
    "        \n",
    "        # Mask all tokens up to and including the prompt (before assistant response)\n",
    "        prompt_length = len(prompt_tokens)\n",
    "        labels[:prompt_length] = -100\n",
    "\n",
    "        return {\n",
    "            \"audio_values\": audio_values,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model to overfit on small dataset\n",
    "\n",
    "We've implemented model & dataset, now we need to test it:\n",
    "* Instantiate model\n",
    "* Move it to device\n",
    "* Create dataset with small number of samples (10, for example)\n",
    "* Create trainer according to seminar notebook\n",
    "* Adjust hyperparams to reach `loss < 0.2`\n",
    "\n",
    "##### Grading Criteria:\n",
    "* **+1 point:** Reaching `loss < 0.2` on your overfit experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model instantiation\n",
    "config = ModelConfig()\n",
    "model = AudioLLM(config)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    dtype = torch.bfloat16\n",
    "\n",
    "    # Move all components to GPU\n",
    "    model.llm = model.llm.to(device=device)\n",
    "    model.whisper_encoder = model.whisper_encoder.to(device=device, dtype=dtype)\n",
    "    model.audio_adapter = model.audio_adapter.to(device=device, dtype=dtype)\n",
    "\n",
    "    print(f\"Model moved to device: {device}, dtype: {dtype}\")\n",
    "else:\n",
    "    print(\"Using CPU (no CUDA available)\")\n",
    "\n",
    "\n",
    "tokenizer = model.tokenizer\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(config.whisper_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = AudioInstructDataset(\n",
    "    json_path = 'instruct_data.json', # Path to your QA-saved Data with specified format,\n",
    "    tokenizer = tokenizer,\n",
    "    feature_extractor = feature_extractor,\n",
    "    num_samples = 10,  # Use only 10 samples for overfitting experiment\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Use Trainer to overfit on small sample\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Use Trainer to overfit on small sample\n",
    "\n",
    "# Freeze all parameters except audio adapter\n",
    "for param in model.whisper_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.llm.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.audio_adapter.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "print(f'Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}')\n",
    "print(f'Total parameters: {sum(p.numel() for p in model.parameters())}')\n",
    "\n",
    "# Training arguments for overfitting\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./overfit_experiment',\n",
    "    max_steps=200,\n",
    "    per_device_train_batch_size=1,\n",
    "    learning_rate=1e-3,\n",
    "    warmup_steps=20,\n",
    "    logging_steps=10,\n",
    "    save_steps=100000,\n",
    "    eval_strategy='no',\n",
    "    bf16=torch.cuda.is_available(),\n",
    "    dataloader_num_workers=0,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=['tensorboard'],\n",
    "    logging_dir='./overfit_experiment/logs',\n",
    ")\n",
    "\n",
    "class AudioLLMTrainer(Trainer):\n",
    "    \"\"\"Custom Trainer for AudioLLM that handles device placement correctly.\"\"\"\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "trainer = AudioLLMTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Plot training loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get training history from trainer\n",
    "training_history = trainer.state.log_history\n",
    "\n",
    "# Extract loss values and steps\n",
    "steps = [entry['step'] for entry in training_history if 'loss' in entry]\n",
    "losses = [entry['loss'] for entry in training_history if 'loss' in entry]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(steps, losses, marker='o', markersize=3)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f'Final loss: {losses[-1] if losses else \"N/A\"}')\n",
    "print(f'Min loss: {min(losses) if losses else \"N/A\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. Pretrained AudioLLM Evaluation (4 points)\n",
    "\n",
    "In this section you'll need to: \n",
    "* Load pretrained checkpoint with architecture you've implemented\n",
    "\n",
    "* Write generation method for the model\n",
    "\n",
    "* Measure the quality of the model on MMLU dataset with *answer probability* - based accuracy\n",
    "\n",
    "\n",
    "#### Grading Criteria:\n",
    "* **+2 points:** Completing `generate` function and meaningfull answers on example audios\n",
    "* **+2 point:** Completed *answer probability* - based accuracy calculation on mmlu_speech subset and reaching `accuracy > 0.4`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation function\n",
    "First, you'll need to:\n",
    "* Implement `get_audio_values` and `generate` functions\n",
    "* Pass example audios into them with or without prompts and demonstrate meaningfull answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Downloading audios we will use in examples\n",
    "\n",
    "! gdown https://drive.google.com/uc?id=18-9If-0WZH0cPZ9MpQEwCrABA3BrTKBh\n",
    "! gdown https://drive.google.com/uc?id=1Yv6B3BEMjjmkcK4ogcutyf6gR1QOl3kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.Audio('gagarin_eng.mp3')\n",
    "# When did Gagarin fly into space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.Audio('gagarin.mp3')\n",
    "# –í –∫–∞–∫–æ–º –≥–æ–¥—É –ì–∞–≥–∞—Ä–∏–Ω –ø–æ–ª–µ—Ç–µ–ª –≤ –∫–æ—Å–º–æ—Å?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model instantiation\n",
    "config = ModelConfig(\n",
    "    whisper_model='openai/whisper-medium',\n",
    "    llm_model=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    adapter_hidden_dim=1536,\n",
    "    adapter_num_layers=2,\n",
    "    adapter_dropout=0.1,\n",
    "    subsample_factor=4,\n",
    ")\n",
    "model = AudioLLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "ckpt_path = hf_hub_download(\n",
    "    repo_id=\"malex26/dls-course-model\",\n",
    "    filename=\"model.ckpt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "model.load_state_dict(ckpt)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_audio_values(audio_path: str | Path, features_extractor: WhisperFeatureExtractor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extracting audio_values tensor from given audio_path\n",
    "    \"\"\"\n",
    "    # Load audio using librosa and resample to 16kHz\n",
    "    audio, sr = librosa.load(audio_path, sr=16000)\n",
    "    \n",
    "    # Process audio with Whisper feature extractor\n",
    "    audio_inputs = features_extractor(\n",
    "        audio,\n",
    "        sampling_rate=16000,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    audio_values = audio_inputs.input_features  # [1, 128, time_frames]\n",
    "    \n",
    "    return audio_values\n",
    "\n",
    "def generate(\n",
    "    model: AudioLLM, \n",
    "    audio_values: torch.Tensor,\n",
    "    user_prompt: str = \"\",\n",
    "    system_prompt: str = \"\",\n",
    "    max_new_tokens: int=128,\n",
    "    **generation_kwargs,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Standard generation logic, but with addition of Audio Embeddings\n",
    "    \"\"\"\n",
    "    model = model.eval()\n",
    "    device = model.get_device()\n",
    "    \n",
    "    if not AUDIO_TOKEN in user_prompt:\n",
    "        # model was trained with AUDIO_TOKEN before user text\n",
    "        user_prompt = AUDIO_TOKEN + user_prompt\n",
    "\n",
    "    # Create the prompt using chat template\n",
    "    text = create_prompt(\n",
    "        model.tokenizer,\n",
    "        instruction=user_prompt,\n",
    "        response=None,  # No response for generation\n",
    "        system_message=system_prompt if system_prompt else BASE_SYSTEM_MESSAGE,\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized = model.tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = tokenized.input_ids.to(device)\n",
    "\n",
    "    with torch.no_grad(), torch.autocast(dtype=torch.bfloat16, device_type='cuda'):\n",
    "        # Obtain audio embeds\n",
    "        audio_values = audio_values.to(device)\n",
    "        audio_embeds = model.encode_audio(audio_values)  # [1, num_audio_tokens, llm_dim]\n",
    "\n",
    "        # Obtain text embeds\n",
    "        text_embeds = model.llm.get_input_embeddings()(input_ids)  # [1, text_seq_len, llm_dim]\n",
    "\n",
    "        # Combine them\n",
    "        combined_embeds, _ = model.insert_audio_embeds(\n",
    "            audio_embeds, text_embeds, input_ids, None, model.audio_token_id\n",
    "        )\n",
    "\n",
    "        # Create attention mask\n",
    "        attention_mask = torch.ones(\n",
    "            1, combined_embeds.shape[1],\n",
    "            device=audio_embeds.device,\n",
    "            dtype=torch.long\n",
    "        )\n",
    "\n",
    "        # Generate\n",
    "        outputs = model.llm.generate(\n",
    "            inputs_embeds=combined_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=model.tokenizer.pad_token_id,\n",
    "            eos_token_id=model.tokenizer.eos_token_id,\n",
    "            **generation_kwargs\n",
    "        )\n",
    "\n",
    "        # Decode tokens\n",
    "        generated_text = model.tokenizer.decode(\n",
    "            outputs[0],\n",
    "            skip_special_tokens=True\n",
    "        ).strip()\n",
    "\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "audio_values = get_audio_values('gagarin_eng.mp3', feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "res = generate(\n",
    "    model=model,\n",
    "    audio_values=audio_values,\n",
    "    user_prompt=f'{AUDIO_TOKEN}Answer the question on audio.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grading Criteria:\n",
    "* **2 points:** Completed `generate` & `get_audio_values` function and demonstrated meaningfull answers on example audios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Measure the quality of the model on MMLU dataset with *answer probability* - based accuracy\n",
    "In this punct you'll evaluate the pretrained AudioLLM model on MMLU-Speech using **probability-based answer extraction**, similar to the method in original [MMLU paper](https://arxiv.org/pdf/2009.03300).\n",
    "\n",
    "### Method\n",
    "\n",
    "Instead of generating and parsing text, directly extract the answer by:\n",
    "\n",
    "1. Create prompt ending with `\"Answer:\"`\n",
    "2. Get model's logits for the **next token**\n",
    "3. Extract probabilities for tokens `\" A\"`, `\" B\"`, `\" C\"`, `\" D\"` (note: with leading space)\n",
    "4. Select the option with **highest probability**\n",
    "\n",
    "**Your goal is**:\n",
    "* implement this evaluation method\n",
    "* measure quality on mmlu-speech dataset and reach `accuracy > 0.4` on the `high_school_psychology` subset\n",
    "\n",
    "#### Grading Criteria:\n",
    "* **2 points:** Creating probability-based validation function and obtained `accuracy > 0.4` on the `high_school_psychology` subset\n",
    "* **1 points:** Creating any validation function and obtained `accuracy > 0.3` on the `high_school_psychology` subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "! pip install datasets==2.21.0 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Dataset downloading, takes about 5 minutes in kaggle notebooks\n",
    "ds = load_dataset(\"mistralai/mmlu_speech\", split='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Filter to high_school_psychology subset\n",
    "ds_psychology = ds.filter(lambda x: x['subject'] == 'high_school_psychology')\n",
    "print(f'Number of high_school_psychology samples: {len(ds_psychology)}')\n",
    "\n",
    "# Define answer tokens (with leading space as per MMLU paper)\n",
    "ANSWER_TOKENS = [' A', ' B', ' C', ' D']\n",
    "answer_token_ids = [model.tokenizer.encode(tok, add_special_tokens=False)[0] for tok in ANSWER_TOKENS]\n",
    "print(f'Answer token IDs: {dict(zip(ANSWER_TOKENS, answer_token_ids))}')\n",
    "\n",
    "def evaluate_mmlu_sample_probability(\n",
    "    model: AudioLLM,\n",
    "    sample: dict,\n",
    "    feature_extractor: WhisperFeatureExtractor,\n",
    ") -> tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Evaluate a single MMLU sample using probability-based answer extraction.\n",
    "    \n",
    "    Returns:\n",
    "        (predicted_answer_idx, correct_answer_idx)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = model.get_device()\n",
    "    \n",
    "    # Get audio from sample\n",
    "    audio_array = sample['audio']['array']\n",
    "    audio_sr = sample['audio']['sampling_rate']\n",
    "    \n",
    "    # Resample if needed\n",
    "    if audio_sr != 16000:\n",
    "        audio_array = librosa.resample(audio_array, orig_sr=audio_sr, target_sr=16000)\n",
    "    \n",
    "    # Process audio with feature extractor\n",
    "    audio_inputs = feature_extractor(\n",
    "        audio_array,\n",
    "        sampling_rate=16000,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    audio_values = audio_inputs.input_features.to(device)\n",
    "    \n",
    "    # Create prompt ending with \"Answer:\"\n",
    "    user_prompt = f\"{AUDIO_TOKEN}Listen to the question and answer choices. Answer:\"\n",
    "    \n",
    "    text = create_prompt(\n",
    "        model.tokenizer,\n",
    "        instruction=user_prompt,\n",
    "        response=None,\n",
    "        system_message=BASE_SYSTEM_MESSAGE,\n",
    "    )\n",
    "    \n",
    "    tokenized = model.tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = tokenized.input_ids.to(device)\n",
    "    \n",
    "    with torch.no_grad(), torch.autocast(dtype=torch.bfloat16, device_type='cuda'):\n",
    "        # Encode audio\n",
    "        audio_embeds = model.encode_audio(audio_values)\n",
    "        \n",
    "        # Get text embeddings\n",
    "        text_embeds = model.llm.get_input_embeddings()(input_ids)\n",
    "        \n",
    "        # Combine embeddings\n",
    "        combined_embeds, _ = model.insert_audio_embeds(\n",
    "            audio_embeds, text_embeds, input_ids, None, model.audio_token_id\n",
    "        )\n",
    "        \n",
    "        # Create attention mask\n",
    "        attention_mask = torch.ones(\n",
    "            1, combined_embeds.shape[1],\n",
    "            device=device,\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        \n",
    "        # Forward pass to get logits\n",
    "        outputs = model.llm(\n",
    "            inputs_embeds=combined_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Get logits for the last position (next token prediction)\n",
    "        last_logits = outputs.logits[0, -1, :]  # [vocab_size]\n",
    "        \n",
    "        # Extract probabilities for answer tokens\n",
    "        answer_logits = last_logits[answer_token_ids]\n",
    "        answer_probs = torch.softmax(answer_logits, dim=0)\n",
    "        \n",
    "        # Get predicted answer\n",
    "        predicted_idx = answer_probs.argmax().item()\n",
    "        \n",
    "    # Get correct answer index\n",
    "    correct_idx = sample['answer']\n",
    "    \n",
    "    return predicted_idx, correct_idx\n",
    "\n",
    "# Evaluate on high_school_psychology subset\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for i, sample in enumerate(tqdm(ds_psychology, desc='Evaluating MMLU')):\n",
    "    try:\n",
    "        pred_idx, correct_idx = evaluate_mmlu_sample_probability(\n",
    "            model, sample, feature_extractor\n",
    "        )\n",
    "        if pred_idx == correct_idx:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    except Exception as e:\n",
    "        print(f'Error on sample {i}: {e}')\n",
    "        continue\n",
    "\n",
    "accuracy = correct / total if total > 0 else 0\n",
    "print(f'\\nResults on high_school_psychology subset:')\n",
    "print(f'Correct: {correct}/{total}')\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
